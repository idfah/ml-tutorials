{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the spaCy NLP library\n",
    "\n",
    "---\n",
    "\n",
    "__Elliott Forney - 2020__\n",
    "\n",
    "This notebook is a basic, introductory exploration of the spaCy NLP library.  We will explore how to:\n",
    "\n",
    "* download stock models\n",
    "* perform word and sentence tokenization\n",
    "* extract part-of-speech tags\n",
    "* chunk noun phrases\n",
    "* perform named entity recognition\n",
    "\n",
    "This is mostly for my own enlightenment and is not intended to be a thorough tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[SpaCy](https://spacy.io/) is an [open-source library](https://github.com/explosion/spaCy) for performing a variety of NLP tasks.  SpaCy claims to be an \"industrial strength\" library aimed at real-world and production NLP use cases.\n",
    "\n",
    "Let's begin by importing the spaCy library and any dependencies we might need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading and installing stock models\n",
    "\n",
    "SpaCy comes with a number of stock models, in various languages, that are ready for immediate use.  After browsing through the [online model catelog](https://spacy.io/models), it sounds like we'll be interested in looking at the `en_core_web_sm` model, which is the stock English model, trained over data fetched from the web and with a relatively small footprint.  The model is described by spaCy as an \"English multi-task CNN trained on OntoNotes,\" that \"Assigns context-specific token vectors, POS tags, dependency parse and named entities.\"\n",
    "\n",
    "Note that each model contains multiple pipleine components, i.e., NLP features, packaged together.  The `en_core_web_sm` model, for example, contains tagger, parser and NER components.\n",
    "\n",
    "SpaCy models can be downloaded directly from a shell prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.3.1 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz#egg=en_core_web_sm==2.3.1 in /home/idfah/.local/lib/python3.8/site-packages (2.3.1)\n",
      "Requirement already satisfied: spacy<2.4.0,>=2.3.0 in /home/idfah/.local/lib/python3.8/site-packages (from en_core_web_sm==2.3.1) (2.3.4)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/idfah/.local/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0; python_version >= \"3.6\" in /home/idfah/.local/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.7.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/idfah/.local/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.19.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/idfah/.local/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.4)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /home/idfah/.local/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.1.3)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (41.6.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/idfah/.local/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.50.0)\n",
      "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in /home/idfah/.local/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (7.4.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/idfah/.local/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.4)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /home/idfah/.local/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.4)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /home/idfah/.local/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/idfah/.local/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.24.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /home/idfah/.local/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.8.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/idfah/.local/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/idfah/.local/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/idfah/.local/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/idfah/.local/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.25.10)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Keyring is skipped due to an exception: Failed to unlock the keyring!\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternately, the model can be downloaded directly from a python or ipython prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "spacy.cli.download('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model has been successfully downloaded, it can now be loaded from within python using the `spacy.load` function.\n",
    "\n",
    "Note that we use the variable name `nlp` for the object returned by the model load operation.  This object can be thought of as an \"NLP processing pipeline\" that can be called over text document to apply all of the pipeline components contained in the downloaded model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample document\n",
    "\n",
    "Next, let's load some sample data to play with.  The document that we will look at is a plaintext version of my PhD dissertation; however, any English document would do here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dissertation.txt', mode='r', encoding='utf8') as fh:\n",
    "    data = fh.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to make sure it was loaded correctly, we print the first 1,500 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DISSERTATION\n",
      "\n",
      "CONVOLUTIONAL NEURAL NETWORKS FOR EEG SIGNAL CLASSIFICATION IN\n",
      "ASYNCHRONOUS BRAIN-COMPUTER INTERFACES\n",
      "\n",
      "Submitted by\n",
      "Elliott M. Forney\n",
      "Department of Computer Science\n",
      "\n",
      "In partial fulfillment of the requirements\n",
      "For the Degree of Doctor of Philosophy\n",
      "Colorado State University\n",
      "Fort Collins, Colorado\n",
      "Fall 2019\n",
      "\n",
      "Doctoral Committee:\n",
      "Advisor: Charles Anderson\n",
      "Asa Ben-Hur\n",
      "Michael Kirby\n",
      "Donald Rojas\n",
      "\n",
      "Copyright Elliott M. Forney 2019\n",
      "\n",
      "This work is licensed under the Creative Commons\n",
      "Attribution-NonCommercial-NoDerivatives 4.0 International Public License.\n",
      "\n",
      "You are permitted to share this document without modification for non-commercial purposes.\n",
      "To view a copy of the full license, please see Appendix ??.\n",
      "\n",
      "ABSTRACT\n",
      "\n",
      "CONVOLUTIONAL NEURAL NETWORKS FOR EEG SIGNAL CLASSIFICATION IN\n",
      "ASYNCHRONOUS BRAIN-COMPUTER INTERFACES\n",
      "\n",
      "Brain-Computer Interfaces (BCIs) are emerging technologies that enable users to interact\n",
      "with computerized devices using only voluntary changes in their mental state. BCIs have a\n",
      "number of important applications, especially in the development of assistive technologies for\n",
      "people with motor impairments. Asynchronous BCIs are systems that aim to establish smooth,\n",
      "continuous control of devices like mouse cursors, electric wheelchairs and robotic prostheses\n",
      "without requiring the user to interact with time-locked external stimuli.\n",
      "Scalp-recorded Electroencephalography (EEG) is a noninvasive approach for measuring\n",
      "brain activity that shows considerable potential for \n"
     ]
    }
   ],
   "source": [
    "print(data[:1500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying the pipeline\n",
    "\n",
    "Next, we apply the `nlp` pipeline to out text `data` and store the resulting object in the variable `doc`.\n",
    "\n",
    "This takes a bit, so we also track the time taken to apply the pipeline.  On my machine, this takes about eight seconds.  Note that spaCy does have a [GPU-enabled package that can be installed](https://spacy.io/usage#gpu), but we'll stick to the standard version for now and explore computational performance more deeply on another day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.935683727264404"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "doc = nlp(data)\n",
    "\n",
    "time.time() - start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at tokens\n",
    "\n",
    "Note that the `doc` object is an instance of `spacy.tokens.doc.Doc` and behaves like a sequence of word tokens when iterated over direclty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our document contains about 89,000 words and we can easily access the string representations of these words by iterating over `doc` and accessing the `.text` attribute of the resulting `spacy.tokens.token.Token` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(spacy.tokens.token.Token, 89119)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc[0]), len(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This allows for simple, pythonic expressions to be used to access word tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DISSERTATION',\n",
       " '\\n\\n',\n",
       " 'CONVOLUTIONAL',\n",
       " 'NEURAL',\n",
       " 'NETWORKS',\n",
       " 'FOR',\n",
       " 'EEG',\n",
       " 'SIGNAL',\n",
       " 'CLASSIFICATION',\n",
       " 'IN',\n",
       " '\\n',\n",
       " 'ASYNCHRONOUS',\n",
       " 'BRAIN',\n",
       " '-',\n",
       " 'COMPUTER',\n",
       " 'INTERFACES',\n",
       " '\\n\\n',\n",
       " 'Submitted',\n",
       " 'by',\n",
       " '\\n']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token.text for token in doc[:20]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token offsets\n",
    "\n",
    "The character offset of each token within the original text is stored in the `.idx` attribute of each `Token` object.  The begin and end offsets can then be extracted using the starting offset and the length of each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('DISSERTATION', 0, 12),\n",
       " ('\\n\\n', 12, 14),\n",
       " ('CONVOLUTIONAL', 14, 27),\n",
       " ('NEURAL', 28, 34),\n",
       " ('NETWORKS', 35, 43),\n",
       " ('FOR', 44, 47),\n",
       " ('EEG', 48, 51),\n",
       " ('SIGNAL', 52, 58),\n",
       " ('CLASSIFICATION', 59, 73),\n",
       " ('IN', 74, 76),\n",
       " ('\\n', 76, 77),\n",
       " ('ASYNCHRONOUS', 77, 89),\n",
       " ('BRAIN', 90, 95),\n",
       " ('-', 95, 96),\n",
       " ('COMPUTER', 96, 104),\n",
       " ('INTERFACES', 105, 115),\n",
       " ('\\n\\n', 115, 117),\n",
       " ('Submitted', 117, 126),\n",
       " ('by', 127, 129),\n",
       " ('\\n', 129, 130)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(token.text, token.idx, token.idx + len(token)) for token in doc[:20]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be confirmed by extracting each token from the original document using only the token offsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DISSERTATION',\n",
       " '\\n\\n',\n",
       " 'CONVOLUTIONAL',\n",
       " 'NEURAL',\n",
       " 'NETWORKS',\n",
       " 'FOR',\n",
       " 'EEG',\n",
       " 'SIGNAL',\n",
       " 'CLASSIFICATION',\n",
       " 'IN',\n",
       " '\\n',\n",
       " 'ASYNCHRONOUS',\n",
       " 'BRAIN',\n",
       " '-',\n",
       " 'COMPUTER',\n",
       " 'INTERFACES',\n",
       " '\\n\\n',\n",
       " 'Submitted',\n",
       " 'by',\n",
       " '\\n']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[data[token.idx:token.idx + len(token)] for token in doc[:20]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting part-of-speech tags\n",
    "\n",
    "The predicted Part-Of-Speech (POS) tags can be access using the `.pos_` attribute on each `Token` object.\n",
    "\n",
    "Note that the `.pos_` attribute returns the string representation of the POS tag from the [Universal POS Tag Set](https://universaldependencies.org/docs/u/pos/) while the `.pos` attribute is an integer enum-style representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NOUN',\n",
       " 'SPACE',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'ADP',\n",
       " 'PROPN',\n",
       " 'PROPN',\n",
       " 'VERB',\n",
       " 'ADP',\n",
       " 'SPACE',\n",
       " 'PROPN',\n",
       " 'PROPN',\n",
       " 'PUNCT',\n",
       " 'NOUN',\n",
       " 'PROPN',\n",
       " 'SPACE',\n",
       " 'VERB',\n",
       " 'ADP',\n",
       " 'SPACE']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token.pos_ for token in doc[:20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[92,\n",
       " 103,\n",
       " 92,\n",
       " 92,\n",
       " 92,\n",
       " 85,\n",
       " 96,\n",
       " 96,\n",
       " 100,\n",
       " 85,\n",
       " 103,\n",
       " 96,\n",
       " 96,\n",
       " 97,\n",
       " 92,\n",
       " 96,\n",
       " 103,\n",
       " 100,\n",
       " 85,\n",
       " 103]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token.pos for token in doc[:20]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is straightforward to get tuples containing both the token text along side the corresponding POS tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(DISSERTATION, 'NOUN'),\n",
       " (\n",
       "  , 'SPACE'),\n",
       " (CONVOLUTIONAL, 'NOUN'),\n",
       " (NEURAL, 'NOUN'),\n",
       " (NETWORKS, 'NOUN'),\n",
       " (FOR, 'ADP'),\n",
       " (EEG, 'PROPN'),\n",
       " (SIGNAL, 'PROPN'),\n",
       " (CLASSIFICATION, 'VERB'),\n",
       " (IN, 'ADP'),\n",
       " (, 'SPACE'),\n",
       " (ASYNCHRONOUS, 'PROPN'),\n",
       " (BRAIN, 'PROPN'),\n",
       " (-, 'PUNCT'),\n",
       " (COMPUTER, 'NOUN'),\n",
       " (INTERFACES, 'PROPN'),\n",
       " (\n",
       "  ,\n",
       "  'SPACE'),\n",
       " (Submitted, 'VERB'),\n",
       " (by, 'ADP'),\n",
       " (, 'SPACE')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(token, token.pos_) for token in doc[:20]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token \"is\" and \"like\" attributes\n",
    "\n",
    "Each token also has convenient attributes that denote whether or not it looks like a digit, number, email, et cetra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2019"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_2019 = doc[54]\n",
    "word_2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2019, True, True, False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_2019, word_2019.is_digit, word_2019.like_num, word_2019.like_email"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition (NER)\n",
    "\n",
    "Our pipeline also contained a component for performing NER.  While iterating over the `doc` variable directly operates over tokens, the results from additional pipeline components are generally `Span` object, which represent a series of contiguous tokens, as an attribute that is placed onto the `doc` object.\n",
    "\n",
    "In this case, we have an attribute called `doc.ents` that is a tuple of `Span` objects representing each entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tuple, spacy.tokens.span.Span)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc.ents), type(doc.ents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, pythonic expressions, e.g., list comprehensions, can be used to extract information from this tuple of `Span`'s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[INTERFACES,\n",
       " Elliott M. Forney,\n",
       " Department of Computer Science,\n",
       " the Degree of Doctor of Philosophy,\n",
       " Colorado State University,\n",
       " Fort Collins,\n",
       " Colorado,\n",
       " Fall 2019,\n",
       " Doctoral Committee,\n",
       " Charles Anderson,\n",
       " Asa Ben-Hur,\n",
       " Michael Kirby,\n",
       " Donald Rojas,\n",
       " the Creative Commons,\n",
       " NonCommercial-NoDerivatives,\n",
       " Appendix,\n",
       " EEG,\n",
       " EEG,\n",
       " the Convolutional Neural Network,\n",
       " CNN]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ent for ent in doc.ents[:20]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.label_` attribute contains a string version of the predicted entity type while `.label` attribute holds an integer representation, similar to what we saw for POS tags.\n",
    "\n",
    "Note that entities are not disambiguated, merged or linked.  *It's unclear to me if there is any way to do these things in spaCy?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('INTERFACES', 'ORG', 383),\n",
       " ('Elliott M. Forney\\n', 'PERSON', 380),\n",
       " ('Department of Computer Science', 'ORG', 383),\n",
       " ('the Degree of Doctor of Philosophy\\n', 'WORK_OF_ART', 388),\n",
       " ('Colorado State University', 'ORG', 383),\n",
       " ('Fort Collins', 'GPE', 384),\n",
       " ('Colorado', 'GPE', 384),\n",
       " ('Fall 2019', 'DATE', 391),\n",
       " ('Doctoral Committee', 'ORG', 383),\n",
       " ('Charles Anderson', 'PERSON', 380),\n",
       " ('Asa Ben-Hur', 'PERSON', 380),\n",
       " ('Michael Kirby', 'PERSON', 380),\n",
       " ('Donald Rojas', 'PERSON', 380),\n",
       " ('the Creative Commons', 'ORG', 383),\n",
       " ('NonCommercial-NoDerivatives', 'ORG', 383),\n",
       " ('Appendix', 'GPE', 384),\n",
       " ('EEG', 'ORG', 383),\n",
       " ('EEG', 'ORG', 383),\n",
       " ('the Convolutional Neural Network', 'ORG', 383),\n",
       " ('CNN', 'ORG', 383)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(ent.text, ent.label_, ent.label) for ent in doc.ents[:20]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, pythonic expressions can be used to do things like extract the text for all entities of a given type.\n",
    "\n",
    "`PERSON` entities look pretty good..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Elliott M. Forney\\n',\n",
       " 'Charles Anderson',\n",
       " 'Asa Ben-Hur',\n",
       " 'Michael Kirby',\n",
       " 'Donald Rojas',\n",
       " 'Charles Anderson',\n",
       " 'BCI',\n",
       " 'Bill Gavin',\n",
       " 'Marla Roll',\n",
       " 'Brittany Taylor',\n",
       " 'Jewel Crasta',\n",
       " 'Stephanie Scott',\n",
       " 'Katie Bruegger',\n",
       " 'Kim Teh',\n",
       " 'Stephanie Teh',\n",
       " 'Tomojit Ghosh',\n",
       " 'Glen Forney',\n",
       " 'Nancy Forney',\n",
       " 'Maggie',\n",
       " 'Parker']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ent.text for ent in doc.ents if ent.label_ == 'PERSON'][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ORG` has some misses, especially with abbreviations..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['INTERFACES',\n",
       " 'Department of Computer Science',\n",
       " 'Colorado State University',\n",
       " 'Doctoral Committee',\n",
       " 'the Creative Commons',\n",
       " 'NonCommercial-NoDerivatives',\n",
       " 'EEG',\n",
       " 'EEG',\n",
       " 'the Convolutional Neural Network',\n",
       " 'CNN',\n",
       " 'EEG',\n",
       " 'Time-Delay Neural',\n",
       " 'EEG',\n",
       " 'EEG',\n",
       " 'Fourier',\n",
       " 'EEG',\n",
       " 'EEG',\n",
       " 'CSU',\n",
       " 'Patti Davies',\n",
       " 'EEG']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ent.text for ent in doc.ents if ent.label_ == 'ORG'][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same for `GPE` (geopolitical entities)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Fort Collins',\n",
       " 'Colorado',\n",
       " 'Appendix',\n",
       " 'Colorado',\n",
       " 'distinct mental states',\n",
       " 's2',\n",
       " 'sT',\n",
       " 'DFT',\n",
       " 'Sa\\n\\n',\n",
       " 'noisy',\n",
       " 'CSP-2',\n",
       " 'MI',\n",
       " 'al.',\n",
       " 'al.',\n",
       " '−',\n",
       " '−',\n",
       " 'Mn',\n",
       " 'i.e',\n",
       " 'Unnikrishnan',\n",
       " 'Tzanakou']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ent.text for ent in doc.ents if ent.label_ == 'GPE'][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noun-phrase chunking\n",
    "\n",
    "Noun chunks are also extracted by our pipeline.  This time, however, we are given a generator instead of list, which is certainly reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generator"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc.noun_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(spacy.tokens.span.Span, 17531)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noun_chunks = list(doc.noun_chunks)\n",
    "\n",
    "type(noun_chunks[0]), len(noun_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, noun chunks are easily extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DISSERTATION',\n",
       " 'CONVOLUTIONAL NEURAL NETWORKS',\n",
       " 'EEG SIGNAL',\n",
       " 'ASYNCHRONOUS BRAIN-COMPUTER INTERFACES',\n",
       " 'Elliott M. Forney',\n",
       " 'Department',\n",
       " 'Computer Science',\n",
       " 'partial fulfillment',\n",
       " 'the requirements',\n",
       " 'the Degree',\n",
       " 'Doctor',\n",
       " 'Philosophy',\n",
       " 'Colorado State University',\n",
       " 'Fort Collins',\n",
       " 'Colorado',\n",
       " 'Fall',\n",
       " 'Doctoral Committee',\n",
       " 'Advisor',\n",
       " 'Charles Anderson',\n",
       " 'Asa Ben-Hur']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[noun_chunk.text for noun_chunk in noun_chunks[:20]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence tokenization\n",
    "\n",
    "Sentences are also extracted and, again, are represented as `Span`'s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and I would like to\\nthank you all.',\n",
       " 'I would also like to thank my dissertation committee and all of the members of\\nthe BCI laboratory.',\n",
       " 'Bill Gavin, Patti Davies and Marla Roll, in particular, have taught me much\\nof what I know about EEG and how to work with participants in a kind and professional way.\\n',\n",
       " 'Brittany Taylor, Jewel Crasta, Stephanie Scott, Katie Bruegger, Kim Teh and Stephanie Teh have\\nall been especially instrumental in the success of my research and the BCI project and have all\\nbeen great sources of support and friendship.',\n",
       " 'Tomojit Ghosh and Fereydoon Vafaei have been\\ngreat friends and colleagues and were extremely helpful throughout the process of formulating\\nmy ideas and designing my experiments.',\n",
       " 'I would also like to extend a special thank you to\\neveryone that participated in our studies and, especially, to those who have graciously allowed\\nus to enter their homes.',\n",
       " 'I truly hope that this research helps to develop next-generation\\nassistive technologies for everyone who can benefit from them.',\n",
       " 'This work was supported in\\npart by the National Science Foundation through grant number 1065513 and through\\ngenerous support from the Department of Computer Science, Department of Occupational\\nTherapy and Department of Human Development and Family Studies at Colorado State\\nUniversity.',\n",
       " 'I have also received generous financial support from the Forney Family Scholarship\\nand through the Computer Science Department’s Artificial Intelligence and Evolutionary\\n',\n",
       " 'Computation Fellowship.']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[sent.text for sent in doc.sents][50:60]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By iterating over the tokens contained in each span, we can easily get a nested list of tokens by sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['where',\n",
       "  'T',\n",
       "  'is',\n",
       "  'the',\n",
       "  'number',\n",
       "  'of',\n",
       "  'timesteps',\n",
       "  'in',\n",
       "  'the',\n",
       "  'signal',\n",
       "  'segment',\n",
       "  'and',\n",
       "  's',\n",
       "  't',\n",
       "  'is',\n",
       "  'the',\n",
       "  'signal',\n",
       "  'voltage',\n",
       "  'in',\n",
       "  'µV',\n",
       "  'at',\n",
       "  '\\n',\n",
       "  'time',\n",
       "  't',\n",
       "  '.'],\n",
       " ['The',\n",
       "  'Discrete',\n",
       "  'Fourier',\n",
       "  'Transform',\n",
       "  '(',\n",
       "  'DFT',\n",
       "  ')',\n",
       "  'is',\n",
       "  'then',\n",
       "  'used',\n",
       "  'to',\n",
       "  'represent',\n",
       "  's',\n",
       "  'as',\n",
       "  'a',\n",
       "  'sum',\n",
       "  'of',\n",
       "  'sine',\n",
       "  'and',\n",
       "  '\\n',\n",
       "  'cosine',\n",
       "  'waves',\n",
       "  'in',\n",
       "  'the',\n",
       "  'complex',\n",
       "  'domain.2']]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[token.text for token in sent] for sent in list(doc.sents)[1000:1002]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis\n",
    "\n",
    "Sentiment analysis is contained in some spaCy models, but, unfortunately not the `en_core_web_sm` model.  If it were, however, the sentiment score for the document would be stored in the floating point `.sentiment` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, spaCy seems pretty easy to get started with.  I do wonder a bit if the `Span` data model abstraction, i.e., iterables of tokens, is really sufficient for a wide variety of NLP tasks, but I'll have to ponder on that a bit.  In future notebooks, I hope to explore performance a bit more and also sentiment and training of custom models.  Perhaps I can also think of an easy practical NLP application to integrate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
