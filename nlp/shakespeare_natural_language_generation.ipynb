{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shakespearean Natural Language Generation (NLG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language models work by attempting to continually predict the next word in a large corpus of text.\n",
    "\n",
    "Say that we have a vocabulary, $\\mathbf{V}$, that contains $N$ words and that we also have a phrase consisting of $M$ words, $w_1, w_2, ..., w_M,$ where each word is in $\\mathbf{V}$.  Then, for each timestep, $t$, in our phrase, our model will attempt to predict\n",
    "\n",
    "$$\n",
    "  P(w_t = v_i\\; | \\; w_{t-1}, \\; w_{t-2}, \\; ..., \\; w_1)\n",
    "$$\n",
    "\n",
    "for each word $v_i$ in $\\mathbf{V}$.  In other words, we are continually predicting the probability that each word in our vocabulary will occur next, given a previous sequence of actual observed words.\n",
    "\n",
    "Language models have many uses.  This includes applications in transfer learning for solving various types of NLP problems, such as classification, named entity recognition and sentiment analysis.  Language models can also be used to generate artificial text in a process called Natural Language Generation (NLG).  NLG has various applications, including text generation for deceiving search engines, generating script to be presented by chat bots and creating responses for question answering systems.\n",
    "\n",
    "In this example, we will create a very simple NLG system that mimics Shakespearean text using a relatively simple Elman Recurrent Neural Network (RNN)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, re\n",
    "import munch\n",
    "\n",
    "import nltk\n",
    "import torch as th\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.figsize'] = (9, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we use NLTK to download the `punkt` data, which is used for word-punctuation tokenization, and the `gutenberg` corpora, which includes a number of freely available texts, including several of Shakespeare's plays: Caesar, Hamlet and Macbeth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/idfah/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package gutenberg to /home/idfah/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, 'austen-emma.txt'),\n",
       " (1, 'austen-persuasion.txt'),\n",
       " (2, 'austen-sense.txt'),\n",
       " (3, 'bible-kjv.txt'),\n",
       " (4, 'blake-poems.txt'),\n",
       " (5, 'bryant-stories.txt'),\n",
       " (6, 'burgess-busterbrown.txt'),\n",
       " (7, 'carroll-alice.txt'),\n",
       " (8, 'chesterton-ball.txt'),\n",
       " (9, 'chesterton-brown.txt'),\n",
       " (10, 'chesterton-thursday.txt'),\n",
       " (11, 'edgeworth-parents.txt'),\n",
       " (12, 'melville-moby_dick.txt'),\n",
       " (13, 'milton-paradise.txt'),\n",
       " (14, 'shakespeare-caesar.txt'),\n",
       " (15, 'shakespeare-hamlet.txt'),\n",
       " (16, 'shakespeare-macbeth.txt'),\n",
       " (17, 'whitman-leaves.txt')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('gutenberg')\n",
    "list(enumerate(nltk.corpus.gutenberg.fileids()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then, specifically, extract the filenames for the Shakespeare documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames = nltk.corpus.gutenberg.fileids()[14:17]\n",
    "filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then do some data munging to ensure that we have a list of paragraphs for all combined documents and that each paragraph is a list of words represented as python strings.  Note that we use paragraphs rather than sentences in order to ensure our model can capture some level of cross-sentence dependencies.  This will enable our NLG model to produce more than single sentences at a time.  We also remove very short paragraphs, less than three words, and make everything lower case in order to reduce the size of our vocabulary somewhat.  Note that having a binary indicator of capitalization would also be an easy thing to do in order to handle case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ['[', 'the', 'tragedie', 'of', 'julius', 'caesar', 'by', 'william', 'shakespeare', '1599', ']']\n",
      "1 ['actus', 'primus', '.', 'scoena', 'prima', '.']\n",
      "2 ['enter', 'flauius', ',', 'murellus', ',', 'and', 'certaine', 'commoners', 'ouer', 'the', 'stage', '.']\n",
      "3 ['flauius', '.', 'hence', ':', 'home', 'you', 'idle', 'creatures', ',', 'get', 'you', 'home', ':', 'is', 'this', 'a', 'holiday', '?', 'what', ',', 'know', 'you', 'not', '(', 'being', 'mechanicall', ')', 'you', 'ought', 'not', 'walke', 'vpon', 'a', 'labouring', 'day', ',', 'without', 'the', 'signe', 'of', 'your', 'profession', '?', 'speake', ',', 'what', 'trade', 'art', 'thou', '?', 'car', '.', 'why', 'sir', ',', 'a', 'carpenter']\n",
      "4 ['mur', '.', 'where', 'is', 'thy', 'leather', 'apron', ',', 'and', 'thy', 'rule', '?', 'what', 'dost', 'thou', 'with', 'thy', 'best', 'apparrell', 'on', '?', 'you', 'sir', ',', 'what', 'trade', 'are', 'you', '?', 'cobl', '.', 'truely', 'sir', ',', 'in', 'respect', 'of', 'a', 'fine', 'workman', ',', 'i', 'am', 'but', 'as', 'you', 'would', 'say', ',', 'a', 'cobler']\n",
      "5 ['mur', '.', 'but', 'what', 'trade', 'art', 'thou', '?', 'answer', 'me', 'directly']\n",
      "6 ['cob', '.', 'a', 'trade', 'sir', ',', 'that', 'i', 'hope', 'i', 'may', 'vse', ',', 'with', 'a', 'safe', 'conscience', ',', 'which', 'is', 'indeed', 'sir', ',', 'a', 'mender', 'of', 'bad', 'soules']\n"
     ]
    }
   ],
   "source": [
    "# combine all of the corpora, which are segmented by paragraph, sentences and words\n",
    "paras_by_sent = sum((list(nltk.corpus.gutenberg.paras(filename)) for filename in filenames), [])\n",
    "\n",
    "# combine sentences so each element in paras is a list of words in each paragraph\n",
    "paras = [sum((sent for sent in para), []) for para in paras_by_sent]\n",
    "\n",
    "# filter out very short paragraphs\n",
    "paras = list(filter(lambda para: len(para) > 3, paras))\n",
    "\n",
    "# make everything lower case\n",
    "paras = [[word.lower() for word in para] for para in paras]\n",
    "\n",
    "# print a few paragraphs\n",
    "for i, para in enumerate(paras[:7]):\n",
    "    print(i, para)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It also turns out that not all paragraphs end with proper punctuation, since they are plays, so we add a period to any sentence that ends with a word.  We also add a special `<end>` word to the end of each paragraph so that the network will have a way to denote that a paragraph has terminated and a new one is beginning.\n",
    "\n",
    "Finally, we sort the paragraphs by length.  This ensures that paragraphs with similar length will be grouped together when generating minibatches for training, which, in turn, reduces padding and reduces the computational cost during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ['shout', '.', 'flourish', '.', '<end>']\n",
      "1 ['they', 'stab', 'caesar', '.', '<end>']\n",
      "2 ['low', 'march', 'within', '.', '<end>']\n",
      "3 ['exeunt', '.', 'omnes', '.', '<end>']\n",
      "4 ['exit', 'the', 'ghost', '.', '<end>']\n",
      "5 ['laer', '.', 'farewell', '.', '<end>']\n",
      "6 ['ghost', 'beckens', 'hamlet', '.', '<end>']\n"
     ]
    }
   ],
   "source": [
    "# add a period to the end of paragraphs that end without punctuation\n",
    "word_regex = re.compile(r'\\w')\n",
    "def fix_end_punct(para):\n",
    "    if word_regex.match(para[-1]):\n",
    "        return para + ['.']\n",
    "    return para\n",
    "\n",
    "paras = list(map(fix_end_punct, paras))\n",
    "\n",
    "# add a newline at the end of each paragraph\n",
    "paras = [para + ['<end>',] for para in paras]\n",
    "\n",
    "# sort by length, reduces padding and speeds up training\n",
    "paras = sorted(paras, key=len)\n",
    "\n",
    "# print the first few paragraphs\n",
    "for i, para in enumerate(paras[:7]):\n",
    "    print(i, para)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's print some quick summary statistics.  We have about 2,160 paragraphs with a total of 19,669 words.  A histogram of the paragraph lengths also shows that the vast majority of paragraphs are quite short."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nparas: 2160\n",
      "nwords: 89669\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('nparas:', len(paras))\n",
    "print('nwords:', sum(len(para) for para in paras))\n",
    "\n",
    "plt.hist([len(para) for para in paras], bins=100);\n",
    "plt.xlabel('Paragraph Length');\n",
    "plt.ylabel('Number of Paragraphs');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to define a class that can represent the vocabulary of words in the corpus and map them to unique integer indices.  These indices will be used to look up the words in a word embedding matrix.  This can be easily achieved by extending the python `dict` class and adding a constructor to generate indices from our list of paragraphs.\n",
    "\n",
    "We also add `pad` and `unk` special words that represent padding and unknown tokens, respectively.  Padding words will be used to ensure that we are able to generate batches as rectangular tensors and the unknown token is used when a word that is not in the vocabulary is encountered, i.e., during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndexVocab(dict):\n",
    "    '''A map from string words to unique integer indices.\n",
    "    '''\n",
    "    # special tokens for padding and out-of-vocab words\n",
    "    pad, unk, end = '<pad>', '<unk>', '<end>'\n",
    "    pad_idx, unk_idx, end_idx = tuple(range(3))\n",
    "    \n",
    "    def __init__(self, paras):\n",
    "        '''Initialize a new vocabulary that maps string words to unique\n",
    "        integer indices.  This can be used to look words up in a word embedding.\n",
    "        '''\n",
    "        # create a set of words to make them unique\n",
    "        words = set(word for para in paras for word in para) \\\n",
    "            .difference((self.pad, self.unk, self.end))\n",
    "        \n",
    "        # convert to list and add in special tokens\n",
    "        words = [self.pad, self.unk, self.end] + sorted(list(words))\n",
    "        \n",
    "        # assign indices\n",
    "        idx_vocab = {word: idx for idx, word in enumerate(words)}\n",
    "        super().__init__(idx_vocab)\n",
    "        \n",
    "        # store the reverse map so we can go back to strings\n",
    "        self.reverse = {idx: word for word, idx in self.items()}\n",
    "        \n",
    "    def get(self, word, default=unk_idx):\n",
    "        '''Find the integer index for a given string word.\n",
    "        Returns `unk_idx` if the word is out-of-vocab.\n",
    "        '''\n",
    "        return super().get(word, default)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then generate a vocabulary over all the paragraphs in our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<pad>', 0),\n",
       " ('<unk>', 1),\n",
       " ('<end>', 2),\n",
       " ('!', 3),\n",
       " ('&', 4),\n",
       " (\"'\", 5),\n",
       " ('(', 6),\n",
       " (')', 7),\n",
       " (',', 8),\n",
       " ('-', 9),\n",
       " ('.', 10),\n",
       " ('1', 11),\n",
       " ('1599', 12),\n",
       " ('1603', 13),\n",
       " ('2', 14),\n",
       " ('3', 15),\n",
       " ('4', 16),\n",
       " (':', 17),\n",
       " (':)', 18),\n",
       " (';', 19),\n",
       " ('?', 20),\n",
       " ('[', 21),\n",
       " (']', 22),\n",
       " ('].', 23),\n",
       " ('a', 24)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = IndexVocab(paras)\n",
    "list(vocab.items())[:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5848, 10, 2530, 10, 2],\n",
       " [6606, 6183, 910, 10, 2],\n",
       " [3894, 4009, 7406, 10, 2],\n",
       " [2269, 10, 4508, 10, 2],\n",
       " [2273, 6576, 2763, 10, 2],\n",
       " [3653, 10, 2357, 10, 2],\n",
       " [2763, 536, 2965, 10, 2],\n",
       " [2762, 10, 6440, 10, 2],\n",
       " [2269, 10, 2746, 10, 2],\n",
       " [24, 4406, 7406, 10, 2],\n",
       " [2181, 24, 4116, 10, 2],\n",
       " [5227, 6576, 3764, 10, 2],\n",
       " [2181, 6868, 1212, 10, 2],\n",
       " [4946, 6696, 4802, 10, 2]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_idxs = [[vocab.get(word) for word in para] for para in paras]\n",
    "all_idxs[:14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2269, 10, 2746, 10, 2], [2530, 10, 2269, 10, 2], [2181, 6868, 1212, 10, 2]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.shuffle(all_idxs)\n",
    "\n",
    "nvalid = 150\n",
    "\n",
    "train_idxs = all_idxs[:-nvalid]\n",
    "valid_idxs = all_idxs[-nvalid:]\n",
    "\n",
    "train_idxs.sort(key=len, reverse=True)\n",
    "valid_idxs.sort(key=len, reverse=True)\n",
    "\n",
    "train_idxs[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLGDataset(th.utils.data.Dataset):\n",
    "    def __init__(self, phrases):\n",
    "        super().__init__()\n",
    "        self.phrases = phrases\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        phrase = self.phrases[idx]\n",
    "        return th.as_tensor(phrase), th.as_tensor(len(phrase))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = NLGDataset(train_idxs)\n",
    "valid_data = NLGDataset(valid_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2181, 6868, 1212,   10,    2]), tensor(5))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(th.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, nhidden):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.nhidden = nhidden\n",
    "        \n",
    "        self.embed = th.nn.Embedding(\n",
    "            num_embeddings=self.vocab_size,\n",
    "            embedding_dim=self.embed_dim,\n",
    "            padding_idx=IndexVocab.pad_idx)\n",
    "        \n",
    "        self.initial_state = th.nn.Parameter(\n",
    "            th.empty(1, self.nhidden),\n",
    "            requires_grad=True)\n",
    "        th.nn.init.xavier_uniform_(self.initial_state)\n",
    "        \n",
    "        self.hidden = th.nn.RNN(\n",
    "            input_size=self.embed_dim,\n",
    "            hidden_size=self.nhidden)\n",
    "        \n",
    "        self.visible = th.nn.Linear(\n",
    "            in_features=self.nhidden,\n",
    "            out_features=self.vocab_size)\n",
    "        \n",
    "    def forward(self, idxs, lens, state=None):\n",
    "        assert idxs.ndim == 2\n",
    "        max_len = idxs.shape[0]\n",
    "        batch_size = idxs.shape[1]\n",
    "        \n",
    "        assert lens.ndim == 1\n",
    "        assert lens.shape[0] == batch_size\n",
    "        \n",
    "        word_vectors = self.embed(idxs)\n",
    "        \n",
    "        assert word_vectors.ndim == 3\n",
    "        assert word_vectors.shape[0] == max_len\n",
    "        assert word_vectors.shape[1] == batch_size\n",
    "        assert word_vectors.shape[2] == self.embed_dim\n",
    "        \n",
    "        if state is None:\n",
    "            state = self.initial_state[None, ...].repeat(1, batch_size, 1)\n",
    "        \n",
    "        packed_word_vectors = th.nn.utils.rnn.pack_padded_sequence(word_vectors, lens)\n",
    "        packed_hidden_out, final_state = self.hidden(packed_word_vectors, hx=state)\n",
    "        hidden_out, _ = th.nn.utils.rnn.pad_packed_sequence(packed_hidden_out)\n",
    "        \n",
    "        assert hidden_out.ndim == 3\n",
    "        assert hidden_out.shape[0] == max_len\n",
    "        assert hidden_out.shape[1] == batch_size\n",
    "        assert hidden_out.shape[2] == self.nhidden\n",
    "        \n",
    "        visible_out = self.visible(hidden_out)\n",
    "        assert visible_out.ndim == 3\n",
    "        assert visible_out.shape[0] == max_len\n",
    "        assert visible_out.shape[1] == batch_size\n",
    "        assert visible_out.shape[2] == self.vocab_size\n",
    "        \n",
    "        return visible_out, final_state\n",
    "    \n",
    "    def labels(self, idxs, lens, state=None):\n",
    "        preds, final_state = self(idxs, lens, state=state)\n",
    "        return th.argmax(preds, dim=2), final_state\n",
    "    \n",
    "    @th.no_grad()\n",
    "    def iterate(self, seed_idxs, max_words=64):\n",
    "        seed_idxs = seed_idxs[:, None]\n",
    "        \n",
    "        one_len = th.ones(1, dtype=th.int64)\n",
    "                \n",
    "        # initial predictions of seed words\n",
    "        init_idxs, state = self.labels(\n",
    "            seed_idxs, th.as_tensor(seed_idxs.shape[0])[None,])\n",
    "                \n",
    "        # list of new predictions, start with first seedword\n",
    "        new_idxs = [seed_idxs,]\n",
    "        \n",
    "        # current prediction, starts with last seedword prediction\n",
    "        idxs = init_idxs[-1][None, :]\n",
    "        for i in range(max_words):\n",
    "            idxs, state = self.labels(\n",
    "                idxs, th.as_tensor(idxs.shape[0])[None,], state=state)\n",
    "            \n",
    "            new_idxs.append(idxs)\n",
    "            \n",
    "            if idxs[0][0].item() == IndexVocab.end_idx:\n",
    "                break\n",
    "                \n",
    "        return th.cat(new_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(data):\n",
    "    phrases, lens = zip(*data)\n",
    "    return th.nn.utils.rnn.pad_sequence(phrases), th.stack(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = th.utils.data.DataLoader(train_data, batch_size=3, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data, valid_data, epochs=30, batch_size=16, learning_rate=0.0015, use_gpu=False):\n",
    "    if use_gpu:\n",
    "        model = model.cuda()\n",
    "    model.train()\n",
    "    \n",
    "    result = munch.Munch({\n",
    "        'train_losses': [],\n",
    "        'valid_losses': [],\n",
    "    })\n",
    "    \n",
    "    train_loader = th.utils.data.DataLoader(\n",
    "        train_data,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collate)\n",
    "    \n",
    "    valid_loader = th.utils.data.DataLoader(\n",
    "        valid_data,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collate)\n",
    "    \n",
    "    loss_func = th.nn.CrossEntropyLoss(reduction='mean')\n",
    "    \n",
    "    optimizer = th.optim.Adam(\n",
    "        filter(lambda param: param.requires_grad, model.parameters()),\n",
    "        lr=learning_rate)\n",
    "    \n",
    "    result.best_valid_loss = float('inf')\n",
    "    result.best_epoch = 0.0\n",
    "    best_weights = None\n",
    "        \n",
    "    for epoch in range(epochs):\n",
    "        train_losses = []\n",
    "        for idxs, lens in train_loader:\n",
    "            if use_gpu:\n",
    "                idxs = idxs.cuda()\n",
    "                lens = lens.cuda()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            preds, _final_state = model(idxs[:-1], lens - 1)\n",
    "            targs = idxs[1:]\n",
    "            \n",
    "            loss = loss_func(\n",
    "                preds.view(-1, preds.shape[-1]),\n",
    "                targs.view(-1))\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_losses.append(loss.item())\n",
    "            \n",
    "            \n",
    "        result.train_losses.append(\n",
    "            sum(train_losses) / len(train_losses))\n",
    "        \n",
    "        valid_losses = []\n",
    "        for idxs, lens in valid_loader:\n",
    "            if use_gpu:\n",
    "                idxs = idxs.cuda()\n",
    "                lens = lens.cuda()\n",
    "            \n",
    "            with th.no_grad():\n",
    "                preds, _final_state = model(idxs[:-1], lens - 1)\n",
    "                targs = idxs[1:]\n",
    "            \n",
    "                loss = loss_func(\n",
    "                    preds.view(-1, preds.shape[-1]),\n",
    "                    targs.view(-1))\n",
    "            \n",
    "            valid_losses.append(loss.item())\n",
    "            \n",
    "        result.valid_losses.append(\n",
    "            sum(valid_losses) / len(valid_losses))\n",
    "        \n",
    "        if epoch % 32 == 0:\n",
    "            print(epoch, 'train_loss:', result.train_losses[-1])\n",
    "            print(epoch, 'valid_loss:', result.valid_losses[-1])\n",
    "\n",
    "        if result.valid_losses[-1] < result.best_valid_loss:\n",
    "            #print('best:', epoch)\n",
    "            result.best_valid_loss = result.valid_losses[-1]\n",
    "            result.best_epoch = epoch\n",
    "            \n",
    "            best_weights = model.state_dict()\n",
    "\n",
    "    model.load_state_dict(best_weights)        \n",
    "    \n",
    "    if use_gpu:\n",
    "        model = model.cpu()\n",
    "    model.eval()\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LanguageModel(\n",
    "    vocab_size=len(vocab),\n",
    "    embed_dim=24,\n",
    "    nhidden=48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 train_loss: 7.73249702226548\n",
      "0 valid_loss: 7.053115248680115\n",
      "32 train_loss: 4.227978978838239\n",
      "32 valid_loss: 5.468559145927429\n"
     ]
    }
   ],
   "source": [
    "result = train(model, train_data, valid_data, epochs=512, batch_size=48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seed_words = ['beware', 'the', 'ides',]\n",
    "#seed_words = ['i', 'not']\n",
    "#seed_words = ['i', 'haue', 'not']\n",
    "#seed_words = ['i', 'haue', 'not', 'made', 'of', 'sheep',]\n",
    "seed_words = ['she', 'is', 'of']\n",
    "\n",
    "seed_idxs = th.as_tensor([vocab.get(word) for word in seed_words])\n",
    "seed_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_preds = model.iterate(seed_idxs)\n",
    "\n",
    "[vocab.reverse[word.item()] for word in iter_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(result.train_losses, linewidth=2, label='train');\n",
    "plt.plot(result.valid_losses, linewidth=2, label='valid');\n",
    "plt.scatter(\n",
    "    (result.best_epoch,), (result.valid_losses[result.best_epoch],),\n",
    "    linewidth=3, s=200, marker='x', color='red', label='best')\n",
    "plt.legend();\n",
    "plt.xlabel('Epoch');\n",
    "plt.ylabel('Mean Loss');\n",
    "plt.grid();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.initial_state.detach().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_state = model.initial_state.detach().squeeze()\n",
    "plt.bar(range(len(init_state)), init_state, width=1);\n",
    "plt.xlabel('Index');\n",
    "plt.ylabel('Initial State Value');\n",
    "plt.autoscale(tight=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
