{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring parallelism and concurrency in Python\n",
    "\n",
    "__Elliott Forney - 2020__\n",
    "\n",
    "---\n",
    "\n",
    "Building software that leverages concurrency and parallelism in Python can be a sophisticated topic.  Furthermore, there are many common misunderstandings about how and the degree to which parallelism can be achieved in Python.  In this talk, we will discuss some of the intricacies of concurrency and parallelism along with simple examples of how to use async, multithreaded and multiprocessing paradigms.  We will also briefly discuss CPython's Global Interpreter Lock (GIL) and we will dispell some of the common misconceptions about how and when it limits parallelism in Python.\n",
    "\n",
    "To summarize, we will examine:\n",
    "\n",
    "1.  Differences between parallelism and concurrency\n",
    "1.  CPython Global Interpreter Lock\n",
    "1.  A sequential toy problem\n",
    "1.  Concurrency with asynchronous programming and coroutines\n",
    "1.  Concurrency and parallelism with threads\n",
    "1.  Concurrency and parallelism with multiprocessing\n",
    "1.  Concurrency and parallelism in C extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallelism and Concurrency\n",
    "\n",
    "---\n",
    "\n",
    "### True or False:  \"Parallelism and concurrency are different words for the same thing.\"\n",
    "\n",
    "__False__:  Parallelism indicates that multiple processes or threads of execution are running *at the same time*, e.g., on multiple cores, CPUs or machines.\n",
    "\n",
    "The term \"concurrent,\" on the other hand, simply implies that things are happening at the same time.  It is possible to have concurrency *with or without* parallelism.  Consider, for example, that when running multiple processes or threads of execution on a machine with a single core, the operating system may *context switch* between threads of execution, giving the *illusion* of parallelism, but not necessarily the speedup.  Think multiple GUIs running on a single-core machine.  In this case, we have concurrency but not parallelism.  There are also other ways to achieve concurrency, including the use of coroutines, which we will examine shortly.  If, however, we have multiple processes or threads running independently on separate CPU cores, we may have *both* concurrency and parallelism.\n",
    "\n",
    "## The infamous Global Interpreter Lock (GIL)\n",
    "\n",
    "---\n",
    "\n",
    "### True or False:  \"Python does not have real multithreading because of the GIL.\"\n",
    "\n",
    "__False__:  Not only does python have threads, they are true operating system level threads and can provide both parallelism and concurrency.  In the CPython implementation of Python (which is the most common) there is a Global Interpreter Lock (GIL) which **prevents multiple threads from holding access to the interpreter at the same time**.\n",
    "\n",
    "There are many situations, however, where the GIL is actually freed and multiple threads are permitted to run simultanuously in Python.  Again, we will see examples of this shortly.\n",
    "\n",
    "---\n",
    "\n",
    "### True or False:  \"The only way to achieve concurrency or parallelism in Python is to fork multiple process.\"\n",
    "\n",
    "__False__:  One way to overcome the limitations of the GIL is to fork multiple Python interpreters that each have their own GIL.  We will demonstrate, however, that this is __far from the only way__ to achieve concurrency or parallelism in Python, depending on your application.\n",
    "\n",
    "### Quick questions,  when would the audience\n",
    "\n",
    "1.  Use async in Python\n",
    "1.  Use threads in Python\n",
    "1.  Use multiprocessing in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A toy problem\n",
    "\n",
    "---\n",
    "\n",
    "In order to begin exploring concurrency and parallelism in Python, let's first construct a toy problem that may have several opportunities for leveraging concurrency and parallelism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For these examples, we will leverage NumPy to create some computationally expensive calls.  We will also leverage the `time` module in order to intentionally introduce some blocking sleeps and to time the results of our solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say that we have an expensive function call that services some sort of requests.  In a microservice world, each call to this function might correspond to a separate call to our microservice.\n",
    "\n",
    "The `service_request` function below is designed to simulate such a request.  This function does several things and, from the perspective of concurrency and parallelism, it is important that we understand each of these steps.\n",
    "\n",
    "1.  First, note that `service_request` performs five iterations.  This may correspond to an iterative, numerical algorithm that must perform a number of iterations in order to converge.  This also helps to increase the computational requirements of this toy problem and to help us examine when concurrent execution is occuring.\n",
    "1.  Second, note that there is a blocking call to `time.sleep` in each iteration that lasts for a random amount of time in [0, 1] seconds.  This simulates a __blocking__ function call, which is similar to an I/O request that is reading from disk or, in the microservice world, this simulates a request to a downstream service.\n",
    "1.  Third, each iteration computes the Fast Fourier Transform of a random array of floating point values.  This simulates an action that is computationally expensive, from a numerical standpoint.  Note that the NumPy `fft` function is implemented in C under the hood, but runs in a single thread (more on this later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def service_request(i):\n",
    "    '''This is a toy function that involves both a blocking sleep\n",
    "    operation and a single-threaded computational, FFT operation.'''\n",
    "    # perform five iterations\n",
    "    for j in range(5):\n",
    "        # log that the iteration has begun\n",
    "        print(f'call {i} iteration {j}')\n",
    "\n",
    "        # perform a blocking sleep call ~ [0, 1] seconds\n",
    "        # this simulates an I/O operation or network request\n",
    "        secs = np.random.random()\n",
    "        time.sleep(secs)\n",
    "\n",
    "        # and then we do some heavy CPU computation\n",
    "        # note: that the NumPy FFT uses only a single core\n",
    "        s = np.random.random(4000000)\n",
    "        z = np.fft.fft(s)\n",
    "\n",
    "        # log that the iteration is complete\n",
    "        print(f'end {i} iteration {j}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also create a simple decorated called `@timed` that allows us to report the amount of time that a function call takes, without having to rewrite the code.  If you haven't seen a custom python decorator before, consider reviewing this topic!  They can be very useful.  The details of decorators is not, however, important for the remaining examples, other than to realize that this decorator will print the time taken by a function call in seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "def timed(func):\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        total_time = time.time() - start_time\n",
    "        print('\\x1b[31mtook:\\x1b[0m', np.round(total_time, decimals=4))\n",
    "        return result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Execution\n",
    "\n",
    "---\n",
    "\n",
    "The simpliest way for us to simulate a number of requests is to simply run them sequentially or consecutively from a simple for loop.  For reference, the following example does just that and reports back the total time taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "call 0 iteration 0\n",
      "end 0 iteration 0\n",
      "call 0 iteration 1\n",
      "end 0 iteration 1\n",
      "call 0 iteration 2\n",
      "end 0 iteration 2\n",
      "call 0 iteration 3\n",
      "end 0 iteration 3\n",
      "call 0 iteration 4\n",
      "end 0 iteration 4\n",
      "call 1 iteration 0\n",
      "end 1 iteration 0\n",
      "call 1 iteration 1\n",
      "end 1 iteration 1\n",
      "call 1 iteration 2\n",
      "end 1 iteration 2\n",
      "call 1 iteration 3\n",
      "end 1 iteration 3\n",
      "call 1 iteration 4\n",
      "end 1 iteration 4\n",
      "call 2 iteration 0\n",
      "end 2 iteration 0\n",
      "call 2 iteration 1\n",
      "end 2 iteration 1\n",
      "call 2 iteration 2\n",
      "end 2 iteration 2\n",
      "call 2 iteration 3\n",
      "end 2 iteration 3\n",
      "call 2 iteration 4\n",
      "end 2 iteration 4\n",
      "call 3 iteration 0\n",
      "end 3 iteration 0\n",
      "call 3 iteration 1\n",
      "end 3 iteration 1\n",
      "call 3 iteration 2\n",
      "end 3 iteration 2\n",
      "call 3 iteration 3\n",
      "end 3 iteration 3\n",
      "call 3 iteration 4\n",
      "end 3 iteration 4\n",
      "call 4 iteration 0\n",
      "end 4 iteration 0\n",
      "call 4 iteration 1\n",
      "end 4 iteration 1\n",
      "call 4 iteration 2\n",
      "end 4 iteration 2\n",
      "call 4 iteration 3\n",
      "end 4 iteration 3\n",
      "call 4 iteration 4\n",
      "end 4 iteration 4\n",
      "call 5 iteration 0\n",
      "end 5 iteration 0\n",
      "call 5 iteration 1\n",
      "end 5 iteration 1\n",
      "call 5 iteration 2\n",
      "end 5 iteration 2\n",
      "call 5 iteration 3\n",
      "end 5 iteration 3\n",
      "call 5 iteration 4\n",
      "end 5 iteration 4\n",
      "call 6 iteration 0\n",
      "end 6 iteration 0\n",
      "call 6 iteration 1\n",
      "end 6 iteration 1\n",
      "call 6 iteration 2\n",
      "end 6 iteration 2\n",
      "call 6 iteration 3\n",
      "end 6 iteration 3\n",
      "call 6 iteration 4\n",
      "end 6 iteration 4\n",
      "call 7 iteration 0\n",
      "end 7 iteration 0\n",
      "call 7 iteration 1\n",
      "end 7 iteration 1\n",
      "call 7 iteration 2\n",
      "end 7 iteration 2\n",
      "call 7 iteration 3\n",
      "end 7 iteration 3\n",
      "call 7 iteration 4\n",
      "end 7 iteration 4\n",
      "call 8 iteration 0\n",
      "end 8 iteration 0\n",
      "call 8 iteration 1\n",
      "end 8 iteration 1\n",
      "call 8 iteration 2\n",
      "end 8 iteration 2\n",
      "call 8 iteration 3\n",
      "end 8 iteration 3\n",
      "call 8 iteration 4\n",
      "end 8 iteration 4\n",
      "call 9 iteration 0\n",
      "end 9 iteration 0\n",
      "call 9 iteration 1\n",
      "end 9 iteration 1\n",
      "call 9 iteration 2\n",
      "end 9 iteration 2\n",
      "call 9 iteration 3\n",
      "end 9 iteration 3\n",
      "call 9 iteration 4\n",
      "end 9 iteration 4\n",
      "\u001b[31mtook:\u001b[0m 37.0293\n"
     ]
    }
   ],
   "source": [
    "@timed\n",
    "def run_sequential(n=10):\n",
    "    '''Run `service_request` sequentially `n` times.'''\n",
    "    for i in range(n):\n",
    "        service_request(i)\n",
    "    \n",
    "run_sequential(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asynchronous programming with `asyncio` and coroutines\n",
    "\n",
    "---\n",
    "\n",
    "* Advantage: Provides concurrency and keeps CPU busy during I/O-bound operations.\n",
    "* Disadvantage:  Does not provide CPU parallelism.\n",
    "\n",
    "One way to achieve a performance improvement for this toy problem is to use asynchronous programming.  Asynchronous programming, which was widely popularized by JavaScript, is now natively supported in Python.  The gist of async programming is that we can have multiple functions that are defined with the `async` keyword.  When these functions are called, they return what is called a `future` (similar to a `promise` in JavaScript).  Rather than running executing immediately, we can schedule these `future`s to be run by an event scheduler.  The event scheduler will pick up the next future in its queue and see if it is available to run.  If it is ready to run, it will execute until it yields control back.  If it is not ready to run or when it yields control back to the scheduler, the next available future will be examined.  In this way, we can have many functions, called `coroutines`, running in an interleaved fashion.  This allows us to achieve a level of concurrency.  Async programming does not provide CPU parallelism because only one coroutine is executing on the CPU at any given time.  It can improve performance, however, if a coroutine yields back execution while it is waiting for blocking requests like I/O.\n",
    "\n",
    "In order for an `async` function to yield control back to the scheduler, the `await` keyword must be used (which tells the function to wait for another `future` to complete executing, on another `async` function.  This is best demonstrated by example.  Below, we have an `async` version of our previous `service_request` function.  In addition to adding the `async` keyword to tell Python that this function returns a `future`, we also use `await asynchio.sleep` instead of `time.sleep`.  At this point in the execution of our coroutine, execution will be yielded back to the event scheduler and another coroutine can run the computational portion (the `np.fft`) while another coroutine is sleeping.\n",
    "\n",
    "In the microservice world, this means that we can continue to service requests and to utilize our CPU, even while waiting for a downstream request or while waiting for a read or write operation from storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# many useful async tools are provided by the `asyncio` module\n",
    "import asyncio\n",
    "\n",
    "async def service_request_async(i):\n",
    "    '''This function is the same as `service_request` except that\n",
    "    it supports the asynchronous programming in order to allow\n",
    "    other coroutines to run while it is blocking.'''\n",
    "    for j in range(5):\n",
    "        print(f'call {i} iteration {j}')\n",
    "\n",
    "        # now we use `asyncio.sleep` instead of `time.sleep`.\n",
    "        # this sleep function is `async` so that we can yield\n",
    "        # control of the interpreter to another function while\n",
    "        # it is blocking\n",
    "        secs = np.random.random()\n",
    "        await asyncio.sleep(secs)\n",
    "\n",
    "        # nothing different here\n",
    "        # sice our computation is CPU-bound, i.e., not blocking\n",
    "        # we have nothing to gain from making it `async` because\n",
    "        # we cannot yield control while we wait\n",
    "        s = np.random.random(4000000)\n",
    "        z = np.fft.fft(s)\n",
    "\n",
    "        print(f'end {i} iteration {j}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to execure our async function, we must gather together the futures for all of our calls and then `await` for them to complete.\n",
    "\n",
    "Note that Jupyter notebooks already have an async loop running, so this is all that is necessary.  If this were an ordinary Python program, we would also have to create an event loop and tell it to run until all scheduled futures have completed.\n",
    "\n",
    "Also note that our `@timed` decorator doesn't work in this case because the function immediately returns the futures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "call 0 iteration 0\n",
      "call 1 iteration 0\n",
      "call 2 iteration 0\n",
      "call 3 iteration 0\n",
      "call 4 iteration 0\n",
      "call 5 iteration 0\n",
      "call 6 iteration 0\n",
      "call 7 iteration 0\n",
      "call 8 iteration 0\n",
      "call 9 iteration 0\n",
      "end 0 iteration 0\n",
      "call 0 iteration 1\n",
      "end 8 iteration 0\n",
      "call 8 iteration 1\n",
      "end 0 iteration 1\n",
      "call 0 iteration 2\n",
      "end 4 iteration 0\n",
      "call 4 iteration 1\n",
      "end 7 iteration 0\n",
      "call 7 iteration 1\n",
      "end 5 iteration 0\n",
      "call 5 iteration 1\n",
      "end 9 iteration 0\n",
      "call 9 iteration 1\n",
      "end 1 iteration 0\n",
      "call 1 iteration 1\n",
      "end 2 iteration 0\n",
      "call 2 iteration 1\n",
      "end 6 iteration 0\n",
      "call 6 iteration 1\n",
      "end 3 iteration 0\n",
      "call 3 iteration 1\n",
      "end 7 iteration 1\n",
      "call 7 iteration 2\n",
      "end 8 iteration 1\n",
      "call 8 iteration 2\n",
      "end 0 iteration 2\n",
      "call 0 iteration 3\n",
      "end 4 iteration 1\n",
      "call 4 iteration 2\n",
      "end 1 iteration 1\n",
      "call 1 iteration 2\n",
      "end 5 iteration 1\n",
      "call 5 iteration 2\n",
      "end 9 iteration 1\n",
      "call 9 iteration 2\n",
      "end 3 iteration 1\n",
      "call 3 iteration 2\n",
      "end 6 iteration 1\n",
      "call 6 iteration 2\n",
      "end 2 iteration 1\n",
      "call 2 iteration 2\n",
      "end 7 iteration 2\n",
      "call 7 iteration 3\n",
      "end 0 iteration 3\n",
      "call 0 iteration 4\n",
      "end 8 iteration 2\n",
      "call 8 iteration 3\n",
      "end 5 iteration 2\n",
      "call 5 iteration 3\n",
      "end 4 iteration 2\n",
      "call 4 iteration 3\n",
      "end 1 iteration 2\n",
      "call 1 iteration 3\n",
      "end 3 iteration 2\n",
      "call 3 iteration 3\n",
      "end 9 iteration 2\n",
      "call 9 iteration 3\n",
      "end 2 iteration 2\n",
      "call 2 iteration 3\n",
      "end 6 iteration 2\n",
      "call 6 iteration 3\n",
      "end 7 iteration 3\n",
      "call 7 iteration 4\n",
      "end 8 iteration 3\n",
      "call 8 iteration 4\n",
      "end 5 iteration 3\n",
      "call 5 iteration 4\n",
      "end 0 iteration 4\n",
      "end 4 iteration 3\n",
      "call 4 iteration 4\n",
      "end 1 iteration 3\n",
      "call 1 iteration 4\n",
      "end 3 iteration 3\n",
      "call 3 iteration 4\n",
      "end 9 iteration 3\n",
      "call 9 iteration 4\n",
      "end 6 iteration 3\n",
      "call 6 iteration 4\n",
      "end 2 iteration 3\n",
      "call 2 iteration 4\n",
      "end 7 iteration 4\n",
      "end 8 iteration 4\n",
      "end 5 iteration 4\n",
      "end 1 iteration 4\n",
      "end 4 iteration 4\n",
      "end 3 iteration 4\n",
      "end 6 iteration 4\n",
      "end 9 iteration 4\n",
      "end 2 iteration 4\n",
      "\u001b[31mtook:\u001b[0m 12.4049\n"
     ]
    }
   ],
   "source": [
    "def run_async(n=10):\n",
    "    '''Run our toy problem using the async paradigm.'''\n",
    "    return asyncio.gather(*[service_request_async(i) for i in range(10)])\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "await run_async();\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print('\\x1b[31mtook:\\x1b[0m', np.round(total_time, decimals=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that these requests are now service concurrently and also that the total run time is much better than our sequential example because the CPU can be kept busy during each sleep!\n",
    "\n",
    "Examining `top` while this code executes reveals, however, that only a single CPU is used at a given time.  In other words, we have achieved a level of concurrency, but __not__ parallelism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concurrency and parallelism with threads\n",
    "\n",
    "---\n",
    "\n",
    "* Advantage: shared memory, just like regular threads in C, C++ or Java\n",
    "* Disadvantage: GIL requires that only one thread can access the interpreter at once\n",
    "\n",
    "Python also provides the ability to create threads.  Threads have **shared memory** so that if a variable is changed in one thread, it is also changed in another.  Threads can be executed independently which intrinsically allows them to achieve concurrency.  Threads can also leverage multiple CPUs or CPU cores.\n",
    "\n",
    "Now, as we mentioned before, CPython has a GIL which prevents multiple threads from accessing the interpreter at the same time.  There are, however, many cases when the GIL is actually *released* and multiple threads are allowed to execute in parallel.  Before discussing this further, let's set up a demonstration of our `service_request` example run inside a pool of threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "call 0 iteration 0\n",
      "call 1 iteration 0\n",
      "call 2 iteration 0\n",
      "call 3 iteration 0\n",
      "call 4 iteration 0\n",
      "call 5 iteration 0\n",
      "call 6 iteration 0\n",
      "call 7 iteration 0\n",
      "call 8 iteration 0\n",
      "call 9 iteration 0\n",
      "end 5 iteration 0\n",
      "call 5 iteration 1\n",
      "end 4 iteration 0\n",
      "call 4 iteration 1\n",
      "end 0 iteration 0\n",
      "call 0 iteration 1\n",
      "end 8 iteration 0\n",
      "call 8 iteration 1\n",
      "end 6 iteration 0\n",
      "call 6 iteration 1\n",
      "end 2 iteration 0\n",
      "call 2 iteration 1\n",
      "end 1 iteration 0\n",
      "call 1 iteration 1\n",
      "end 7 iteration 0\n",
      "call 7 iteration 1\n",
      "end 9 iteration 0\n",
      "call 9 iteration 1\n",
      "end 5 iteration 1\n",
      "call 5 iteration 2\n",
      "end 4 iteration 1\n",
      "call 4 iteration 2\n",
      "end 3 iteration 0\n",
      "call 3 iteration 1\n",
      "end 6 iteration 1\n",
      "call 6 iteration 2\n",
      "end 0 iteration 1\n",
      "call 0 iteration 2\n",
      "end 8 iteration 1\n",
      "call 8 iteration 2\n",
      "end 2 iteration 1\n",
      "call 2 iteration 2\n",
      "end 1 iteration 1\n",
      "call 1 iteration 2\n",
      "end 9 iteration 1\n",
      "call 9 iteration 2\n",
      "end 3 iteration 1\n",
      "call 3 iteration 2\n",
      "end 4 iteration 2\n",
      "call 4 iteration 3\n",
      "end 7 iteration 1\n",
      "call 7 iteration 2\n",
      "end 6 iteration 2end 1 iteration 2\n",
      "call 1 iteration 3\n",
      "\n",
      "call 6 iteration 3\n",
      "end 8 iteration 2\n",
      "call 8 iteration 3\n",
      "end 5 iteration 2\n",
      "call 5 iteration 3\n",
      "end 0 iteration 2\n",
      "call 0 iteration 3\n",
      "end 2 iteration 2\n",
      "call 2 iteration 3\n",
      "end 3 iteration 2\n",
      "call 3 iteration 3\n",
      "end 4 iteration 3\n",
      "call 4 iteration 4\n",
      "end 9 iteration 2\n",
      "call 9 iteration 3\n",
      "end 7 iteration 2\n",
      "call 7 iteration 3\n",
      "end 5 iteration 3\n",
      "call 5 iteration 4\n",
      "end 8 iteration 3\n",
      "call 8 iteration 4\n",
      "end 6 iteration 3\n",
      "call 6 iteration 4\n",
      "end 1 iteration 3\n",
      "call 1 iteration 4\n",
      "end 4 iteration 4\n",
      "end 7 iteration 3\n",
      "call 7 iteration 4\n",
      "end 2 iteration 3\n",
      "call 2 iteration 4\n",
      "end 8 iteration 4\n",
      "end 3 iteration 3\n",
      "call 3 iteration 4\n",
      "end 0 iteration 3\n",
      "call 0 iteration 4\n",
      "end 9 iteration 3\n",
      "call 9 iteration 4\n",
      "end 5 iteration 4\n",
      "end 6 iteration 4\n",
      "end 1 iteration 4\n",
      "end 7 iteration 4\n",
      "end 2 iteration 4\n",
      "end 9 iteration 4\n",
      "end 0 iteration 4\n",
      "end 3 iteration 4\n",
      "\u001b[31mtook:\u001b[0m 7.5736\n"
     ]
    }
   ],
   "source": [
    "# the threading module provides thread-related tools\n",
    "import threading\n",
    "\n",
    "@timed\n",
    "def run_threads(n=10):\n",
    "    # create threads using the previous, nonasync `service_request` function\n",
    "    threads = [threading.Thread(target=service_request, args=(i,)) for i in range(10)]\n",
    "\n",
    "    # start them all\n",
    "    for thread in threads:\n",
    "        thread.start()\n",
    "\n",
    "    # wait for them all to finish\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "\n",
    "run_threads()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that our threads execute in an interleaved fashion, which we would expect from concurrency, but also that the run time is much less than even our async implementation!\n",
    "\n",
    "How is this possible since the GIL is supposed to prevent multiple threads from running in parallel?!\n",
    "\n",
    "The answer is that the GIL is released during both the `time.time` and also during the `np.fft` calls, which allows these sections of code to run in parallel on multiple CPUs.  Examining `top` while this code executes will demonstrate that multiple cores are, in fact, in use.\n",
    "\n",
    "As it turns out, many functions in Python that block on I/O and network traffic actually release the GIL while they wait.  Also, C extensions can release the GIL while they are performing computations that do not require access to the interpreter, e.g., `np.fft`.  This means that threads can often give really nice performance improvements in Python, while also providing concurrency, despite the GIL.  Of course, Python code that does not block on I/O and that uses computations in native Python, will likely not see much (if any) parallelism when using threads, due to the GIL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallelism and concurrency with multiprocessing\n",
    "\n",
    "---\n",
    "\n",
    "* Advantage: each process has its own GIL, i.e., multiple interpreters that can be accessed in parallel\n",
    "* Disadvantage: each process has separate memory, must explicitely put things in shared memory, queues or pipes\n",
    "\n",
    "When the GIL limits our ability to achieve parallelism and we are performing heavy computations in native Python, we may also choose to use the `multiprocessing` module.  Multiprocessing is similar to using threads, except that full-blown heavy-weight processes are spawned instead of lightweight threads.  In fact, each process contains a separate copy of the Python interpreter.  This allows us to overcome the limitations of the GIL since each process can access its own interpreter at the same time.  However, this comes at the cost of having to spawn a full-blown process, which takes time, and also not having fully shared memory between processes.  If we want to send information between processes, we have to set up shared memory, or use shared queues or pipes or sockets in order to for our processes to communicate.  This is in contrast to having the ability to simply modify variables or attributes when using threads.\n",
    "\n",
    "Below, we see a similar example to the preivous threading example, except that we are now using processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "call 0 iteration 0\n",
      "call 1 iteration 0\n",
      "call 2 iteration 0\n",
      "call 3 iteration 0\n",
      "call 4 iteration 0\n",
      "call 5 iteration 0\n",
      "call 6 iteration 0\n",
      "call 7 iteration 0\n",
      "call 8 iteration 0\n",
      "call 9 iteration 0\n",
      "end 8 iteration 0\n",
      "end 9 iteration 0\n",
      "call 9 iteration 1\n",
      "end 3 iteration 0\n",
      "call 8 iteration 1\n",
      "call 3 iteration 1\n",
      "end 5 iteration 0\n",
      "call 5 iteration 1\n",
      "end 7 iteration 0\n",
      "call 7 iteration 1\n",
      "end 6 iteration 0\n",
      "call 6 iteration 1\n",
      "end 1 iteration 0\n",
      "call 1 iteration 1\n",
      "end 4 iteration 0\n",
      "end 2 iteration 0\n",
      "call 4 iteration 1\n",
      "call 2 iteration 1\n",
      "end 0 iteration 0\n",
      "call 0 iteration 1\n",
      "end 9 iteration 1\n",
      "call 9 iteration 2\n",
      "end 5 iteration 1\n",
      "call 5 iteration 2\n",
      "end 8 iteration 1\n",
      "end 3 iteration 1\n",
      "call 8 iteration 2\n",
      "call 3 iteration 2\n",
      "end 1 iteration 1\n",
      "call 1 iteration 2\n",
      "end 6 iteration 1\n",
      "call 6 iteration 2\n",
      "end 7 iteration 1\n",
      "call 7 iteration 2\n",
      "end 0 iteration 1\n",
      "call 0 iteration 2\n",
      "end 4 iteration 1\n",
      "call 4 iteration 2\n",
      "end 2 iteration 1\n",
      "call 2 iteration 2\n",
      "end 9 iteration 2\n",
      "call 9 iteration 3\n",
      "end 5 iteration 2\n",
      "call 5 iteration 3\n",
      "end 8 iteration 2\n",
      "call 8 iteration 3\n",
      "end 3 iteration 2\n",
      "call 3 iteration 3\n",
      "end 1 iteration 2\n",
      "call 1 iteration 3\n",
      "end 6 iteration 2\n",
      "call 6 iteration 3\n",
      "end 7 iteration 2\n",
      "call 7 iteration 3\n",
      "end 0 iteration 2\n",
      "call 0 iteration 3\n",
      "end 4 iteration 2\n",
      "call 4 iteration 3\n",
      "end 2 iteration 2\n",
      "call 2 iteration 3\n",
      "end 9 iteration 3\n",
      "call 9 iteration 4\n",
      "end 5 iteration 3\n",
      "end 8 iteration 3\n",
      "call 5 iteration 4\n",
      "call 8 iteration 4\n",
      "end 3 iteration 3\n",
      "call 3 iteration 4\n",
      "end 1 iteration 3\n",
      "call 1 iteration 4\n",
      "end 6 iteration 3\n",
      "call 6 iteration 4\n",
      "end 0 iteration 3\n",
      "call 0 iteration 4\n",
      "end 7 iteration 3\n",
      "call 7 iteration 4\n",
      "end 4 iteration 3\n",
      "call 4 iteration 4\n",
      "end 2 iteration 3\n",
      "call 2 iteration 4\n",
      "end 9 iteration 4\n",
      "end 5 iteration 4\n",
      "end 3 iteration 4\n",
      "end 8 iteration 4\n",
      "end 1 iteration 4\n",
      "end 6 iteration 4\n",
      "end 0 iteration 4\n",
      "end 7 iteration 4\n",
      "end 4 iteration 4\n",
      "end 2 iteration 4\n",
      "\u001b[31mtook:\u001b[0m 9.0327\n"
     ]
    }
   ],
   "source": [
    "# the multiprocessing module is similar to threading but spawns\n",
    "# full-blown processes, which do not share their entire memory space\n",
    "import multiprocessing as mp\n",
    "\n",
    "@timed\n",
    "def run_processes(n=10):\n",
    "    # create threads using the previous, nonasync `service_request` function\n",
    "    processes = [mp.Process(target=service_request, args=(i,)) for i in range(10)]\n",
    "\n",
    "    # start them all\n",
    "    for process in processes:\n",
    "        process.start()\n",
    "\n",
    "    # wait for them all to finish\n",
    "    for process in processes:\n",
    "        process.join()\n",
    "\n",
    "run_processes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that, in this case, the multiprocessing approach is actually slower than the threading approach!  Again, this is because it takes additional time to fork processes and communicate between processes, while our threading approach already achieves significant parallelism because the GIL is release in the most computationally expensive portions of the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallelism and concurrency in C extensions\n",
    "\n",
    "---\n",
    "\n",
    "One final, but **very important** thing to note, is that C extensions can do whatever they want, including releasing the GIL or using their own threading model, e.g., pthreads, OpenMP, CUDA, et cetra.  This means that many computational libraries that utilize C extensions under the hood, are often very performant without requiring any sort of threading or multiprocessing on the Python side.  For example, libraries like NumPy, PyTorch and TensorFlow all utilize multiple cores (or even GPUs) without being limited by the GIL.\n",
    "\n",
    "As a result, it is often appropriate to combine an approach like async in order to provide concurrency and to keep utilization high during blocking requests, with libraries that utilize high-performance threading models from within C extensions.\n",
    "\n",
    "* Advantage: Extremely flexible\n",
    "* Disadvantage: Can only be done in C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
